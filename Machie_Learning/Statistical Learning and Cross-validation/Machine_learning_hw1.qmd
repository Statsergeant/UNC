---
title: "Machine_Learning_HW1"
editor: visual
format:
  html:
    code-fold: true
---

```{r}
#| include: false
library(tidyverse)
library(ggplot2)
library(knitr)
library(MASS)
library(kableExtra)
```

<br> <br>

Open the data set Boston in the R package MASS. The data information is available with ?Boston. The variable crim is considered as a response variable and the other 10 variables except chas, dis and rad are considered as predictors. Also, we remove the 55th observation and then scale all the predictors so that each predictor has a mean of 0 and a standard deviation of 1. So, you can make the predictor x and the response y using the following R codes <br> 

\> data(Boston) <br> 
\> y <- Boston\[-55, 1\] <br> 
\> x <- Boston\[-55, -c(1,4,8,9)\] <br> 
\> x <- as.matrix(scale(x))<br>

In this problem, we want to find the best model that can predict the crime rate in Boston. Use the following R codes to randomly select a training set including 450 observations. <br> 

\> RNGkind(sample.kind = "Rounding") <br> 
\> set.seed(123) <br> 
\> tran <- sample(nrow(x), 450) <br>

The other 55 observations are considered as a test set.

## Question 1

For each predictor, apply a polynomial regression model with the kth degree, where k = 1, 2, . . . , 15. You should use the R function poly(..., k). Since you have a total of 10 predictors, you can have 10 polynomial regression models where each model has 15 different degrees of the polynomial. For each model, find the optimal value of k that can minimize the prediction error (PE) of the test set. The PE is defined as

The prediction error (PE) is given by:

$$
\text{PE} = \sqrt{\frac{1}{m} \sum_{i=1}^{m} \left( y_i - \hat{f}(x_i) \right)^2 }
$$

where $\hat{f}(\cdot)$ is a fitted polynomial regression using the training set and $m = 55$.

For each model (predictor), provide the optimal value of k and the numerical value of the corresponding PE.

```{r}
#| echo: false
#| warning: false
#| cache: true

data(Boston)
y <- Boston[-55, 1]
x <- Boston[-55, -c(1,4,8,9)] 
x <- as.matrix(scale(x))

RNGkind(sample.kind = "Rounding") 
set.seed(123) 
tran <- sample(nrow(x), 450)

```

```{r}
#| echo: false
#| cache: true
# for each predictor with poly regreesion
model <- 10
df <- 15
pe.poly <- matrix(0,model,df)
for(i in 1:model){
 for(j in 1:df){
 m.poly <- lm(y~poly(x[,i],j),subset=tran)
 pe.poly[i,j] <- sqrt(mean((y - predict(m.poly,as.data.frame(x[,i])))[-tran]^2))
 }
}

optimal.min <- apply(pe.poly, 1, min)

optimal.min.pos <- apply(pe.poly, 1, function(x) which(x == min(x)))

PR1_table <- cbind(optimal.min.pos,optimal.min)

row.names(PR1_table) <- paste0('Model',1:10)
colnames(PR1_table) <- c('position', 'value')
PR1_table %>% kable() %>% kable_styling(full_width = F)
```

## Question 2

For each predictor, apply smoothing spline with the degree of freedoms $k$ from 2 to 15, i.e., $k = 2, 3, \dots, 15$. You can use the \texttt{R} function \texttt{smooth.spline(..., df= )}, which has been discussed in class. Find the optimal value of $k$ that can minimize the prediction error (PE) of the test set. For each model (predictor), provide the optimal value of $k$ and the numerical value of the corresponding PE.

```{r}
#| cache: true
pe.spline <- matrix(0,model,df)
for(i in 1:model){
 for(j in 2:df){
 m.spline <- smooth.spline(x[tran,i],y[tran],df=j)
 pe.spline[i,j] <- sqrt(mean((y-predict(m.spline,x[,i])$y)[-tran]^2))
 }
}
q2.pe.spline <- apply(pe.spline[,-1],1,min)
q2.pos.spline <- apply(pe.spline[,-1],1,function(x) which(x==min(x)))+1
q2.ans <- cbind(q2.pos.spline,q2.pe.spline)
colnames(q2.ans) <- c('k','PE')
rownames(q2.ans) <- paste0('Model',1:10)
q2.ans %>% kable() %>% kable_styling(full_width =F)
```

## Question 3

Next, we apply a step function to each of the 10 predictors, where we break the range of $x$ into bins and fit a different constant in each bin. The step function basically converts a continuous variable into an ordered categorical variable. For example, we construct $k + 1$ new variables for the $j$-th predictor $X_j$ such that:

$$
C_0(X_j) = I(X_j < c_1), \quad C_1(X_j) = I(c_1 \leq X_j < c_2), \quad C_2(X_j) = I(c_2 \leq X_j < c_3), \dots, \quad C_k(X_j) = I(X_j \geq c_k),
$$

where $c_1, c_2, \dots, c_k$ are $k$ cutpoints and $I(\cdot)$ is an indicator function. Note that

$$
C_0(X_j) + C_1(X_j) + \dots + C_k(X_j) = 1,
$$

since $X_j$ must be in exactly one of the $k + 1$ intervals. The \texttt{R} function \texttt{cut(x[,j], k)} can generate $k$ intervals of the $j$th predictor of $x$. This function will give you the numerical values of $k$ cutpoints. Consider $k = 2, 3, \dots, 15$ for each predictor. We then consider $C_1(X_j)$, $C_2(X_j)$, \dots, $C_k(X_j)$ as predictors of a linear model, so:

$$
y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i) + \dots + \beta_k C_k(x_i) + \epsilon_i,
$$

where $\beta_0$ is the mean value of $y_i$ for $X_j < c_1$. Therefore, you can use the \texttt{R} function \texttt{lm(y \sim cut(x[,j], k), subset=tran)} to fit the linear model with $k$ categorical variables based on the training set. For each model (predictor), provide the optimal value of $k$ and the numerical value of the corresponding PE.

```{r}
#| cache: true
pe.step <- matrix(0,model,df)
for(i in 1:model){
 for(j in 2:df){
 step.temp <- cut(x[,i],j)
 m.step <- lm(y~step.temp,subset=tran)
 pe.step[i,j] <- sqrt(mean((y-predict(m.step,as.data.frame(step.temp)))[-tran]^2))
 }
}
q3.pe.step <- apply(pe.step[,-1],1,min)
q3.pos.step <- apply(pe.step[,-1],1,function(x) which(x==min(x)))+1
q3.ans <- cbind(q3.pos.step,q3.pe.step)
colnames(q3.ans) <- c('k','PE')
rownames(q3.ans) <- paste0('Model',1:10)
q3.ans %>% kable() %>% kable_styling(full_width = F)

```

## Question 4

You have a total of 30 models so far (10 models from each question) and the corresponding 30 PEs. You can pick up the best model among the 30 models in terms of the smallest PE. However, this result is based on the validation set approach (450 training sets and 55 test sets). Next, repeat Q1, Q2, and Q3 to obtain the optimal $k$ and PE from 30 models using the 10-fold cross-validation, instead of the validation set approach. You must use the following \texttt{R} code to generate 10 folds.

```{=tex}
\begin{verbatim}
> RNGkind(sample.kind = "Rounding")
> set.seed(123)
> u <- sample(rep(seq(10), length=length(y)))
\end{verbatim}
```
For 30 models, provide the PE along with the optimal value of $k$ using the following table:

$$
\begin{array}{c|c|c|c|c}
 & \text{Q1} & \text{Q2} & \text{Q3} \\
\hline
\text{Model 1} & k & \text{PE} & k & \text{PE} & k & \text{PE} \\
\text{Model 2} & \dots & \dots & \dots & \dots & \dots & \dots \\
\dots & \dots & \dots & \dots & \dots & \dots & \dots \\
\text{Model 9} & \dots & \dots & \dots & \dots & \dots & \dots \\
\text{Model 10} & \dots & \dots & \dots & \dots & \dots & \dots \\
\end{array}
$$

Note that Model $j$ means that you use only the $j$th predictor of $X$ in your model.

```{r}
#| cache: true
#| warning: false


RNGkind(sample.kind = "Rounding")
set.seed(123)
u <- sample(rep(seq(10), length=length(y)))
K <- 10
q4.pe.poly <- q4.pe.spline <- q4.pe.step <- array(0,c(model,df,K))
for(k in 1:K){
 tran <- which(u!=k)
 test <- which(u==k)
 for(i in 1:model){
 for(j in 1:df){
 m.poly <- lm(y~poly(x[,i],j),subset=tran)
 q4.pe.poly[i,j,k] <- sqrt(mean((y - predict(m.poly,as.data.frame(x[,i])))[-tran]^2))
 }
 for(j in 2:df){
 m.spline <- smooth.spline(x[tran,i],y[tran],df=j)
 q4.pe.spline[i,j,k] <- sqrt(mean((y-predict(m.spline,x[,i])$y)[-tran]^2))
 step.temp <- cut(x[,i],j)
 pos.zero.tran <- which(table(step.temp[tran])==0)
 pos.test <- table(step.temp[test])[pos.zero.tran]
 if(sum(pos.test!=0)!=0){
 q4.pe.step[i,j,k] <- NA
 }else{
 m.step <- lm(y~step.temp,subset=tran)
 q4.pe.step[i,j,k] <-
sqrt(mean((y-predict(m.step,as.data.frame(step.temp)))[-tran]^2))
 }
 }
 }
}
pe.ave.poly <- apply(q4.pe.poly,c(1,2),mean)
pe.poly.res <- cbind(apply(pe.ave.poly,1,function(x)
which(x==min(x))),apply(pe.ave.poly,1,min))
pe.ave.spline <- apply(q4.pe.spline,c(1,2),mean)
pe.spline.res <- cbind(apply(pe.ave.spline[,-1],1,function(x)
which(x==min(x)))+1,apply(pe.ave.spline[,-1],1,min))
pe.ave.step <- apply(q4.pe.step,c(1,2),function(x) mean(x,na.rm=TRUE))
pe.step.res <- cbind(apply(pe.ave.step[,-1],1,function(x)
which.min(x))+1,apply(pe.ave.step[,-1],1,function(x) min(x,na.rm=TRUE)))
q4.ans <- cbind(pe.poly.res,pe.spline.res,pe.step.res)
colnames(q4.ans) <- rep(c('k','PE'),3)
rownames(q4.ans) <- paste0('Model',1:10)
q4.ans %>% kable() %>% kable_styling(full_width = F)
```

## Question 5

Which model among the 30 models is the best model when you use the 10-fold cross-validation? How about the validation set approach? Did they pick up the same model as the best model?

***Answer) The best model when we use 10-fold cross-validation is the step function (model 7) with k equals to 13. The best model when we only use validation set is the step function (model 2) when k equals to 10.***
